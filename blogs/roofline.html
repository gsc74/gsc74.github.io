<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Roofline Analysis: Matrix Multiplication</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background: #f4f4f9;
            color: #333;
        }
        header {
            background: #fff;
            color: #333;
            padding: 1rem 0;
            text-align: center;
            border-bottom: 2px solid #ccc;
        }
        .container {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1rem;
            background: #fff;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
        }
        h1, h2, h3 {
            color: #35495e;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1rem auto;
        }
        pre {
            background: #f4f4f9;
            padding: 1rem;
            border-left: 5px solid #35495e;
            overflow-x: auto;
        }
        .footer {
            text-align: center;
            margin-top: 2rem;
            padding: 1rem 0;
            background: #35495e;
            color: #fff;
        }
    </style>
</head>
<body>
    <header>
        <h1>Roofline Analysis of Matrix Multiplication (INT32)</h1>
    </header>
    <div class="container">
        <h2>Introduction</h2>
        <p>
            Roofline analysis is a visual model for evaluating the performance of computational workloads, particularly matrix multiplication. In this blog, we analyze the performance of naive and SIMD (Single Instruction Multiple Data) implementations of matrix multiplication for 32-bit integers (INT32) on a dual-socket Intel Xeon Gold 6248R processor.
        </p>

        <h2>Hardware Specifications and Compute Hardware AI</h2>
        <p>
            The experiments were conducted on a dual-socket Intel Xeon Gold 6248R processor with the following specifications:
        </p>
        <ul>
            <li><strong>Cores:</strong> 24 cores per socket, 48 cores in total.</li>
            <li><strong>Base Frequency:</strong> 3.0 GHz.</li>
            <li><strong>Peak Performance:</strong> 288 GFLOPS (32-bit integer operations).</li>
            <li><strong>Memory Bandwidth:</strong> 281.4 GB/s (dual-socket).</li>
        </ul>
        <p>
            The hardware Arithmetic Intensity (AI) is given by:
            <br>
            <code>Hardware AI = Compute Roof / Memory Bandwidth</code>
        </p>
        <p>
            Substituting the values:
            <br>
            <code>Hardware AI = 288 GFLOPS / 281.4 GB/s ≈ 1.02 FLOPs/byte</code>
        </p>

        <h2>Problem Setup and Theoretical AI</h2>
        <p>
            The problem involves multiplying two square matrices of size <code>N = 8192</code>. Each element in the resulting matrix involves:
        </p>
        <ul>
            <li><strong>8192 multiplications</strong></li>
            <li><strong>8191 additions</strong></li>
        </ul>
        <p>
            Total FLOPs for the operation:
        </p>
        <pre>
FLOPs = 2 × N³ = 2 × 8192³ = 1.099 trillion FLOPs
        </pre>
        <p>
            Bytes Transferred:
        </p>
        <pre>
Matrix A: 8192 × 8192 × 4 bytes
Matrix B: 8192 × 8192 × 4 bytes
Matrix C: 8192 × 8192 × 4 bytes
Total Bytes = 4.39 TB
        </pre>
        <p>
            Theoretical AI based on the problem:
            <br>
            <code>Theoretical AI = FLOPs / Bytes Transferred = 1365.33 FLOPs/byte</code>
        </p>

        <h2>Naive Matrix Multiplication</h2>
        <p>
            The naive implementation uses three nested loops:
        </p>
        <pre>
for (int i = 0; i < N; i++) {
    for (int j = 0; j < N; j++) {
        int32_t sum = 0;
        for (int k = 0; k < N; k++) {
            sum += A[i * N + k] * B[k * N + j];
        }
        C[i * N + j] = sum;
    }
}
        </pre>
        <p>
            Performance metrics for the naive implementation:
            <br>
            <code>Naive GFLOPS/sec = 5.7 GFLOPS</code>
            <br>
            <code>Naive AI = 0.249985 FLOPs/byte</code>
        </p>

        <h2>SIMD Matrix Multiplication</h2>
        <p>
            The SIMD implementation leverages AVX-512 to process 16 elements simultaneously:
        </p>
        <pre>
for (int k = 0; k < N; k += 16) {
    __m512i a = _mm512_loadu_si512(&A[i * N + k]);
    __m512i b = _mm512_loadu_si512(&B[k * N + j]);
    __m512i prod = _mm512_mullo_epi32(a, b);
    c = _mm512_add_epi32(c, prod);
}
        </pre>
        <p>
            Performance metrics for the SIMD implementation:
            <br>
            <code>SIMD GFLOPS/sec = 45.6 GFLOPS</code>
            <br>
            <code>SIMD AI = 0.249985 FLOPs/byte</code>
        </p>

        <h2>Why SIMD is Faster Despite Same AI</h2>
        <p>
            Both naive and SIMD implementations have the same Arithmetic Intensity (AI) of 0.249985 FLOPs/byte. This is because the amount of computation (FLOPs) and memory transfers (bytes) are identical in both cases. However, SIMD achieves significantly better performance in GFLOPS/sec due to its ability to utilize compute resources more efficiently:
        </p>
        <ul>
            <li><strong>Parallelism:</strong> SIMD leverages AVX-512 vector instructions, which can perform 16 operations per cycle, compared to scalar instructions in the naive implementation.</li>
            <li><strong>Reduced Compute Bottlenecks:</strong> By executing multiple operations in a single instruction, SIMD minimizes the time spent on computation, resulting in faster runtimes.</li>
            <li><strong>Compute Efficiency:</strong> SIMD achieves higher throughput within the same memory bandwidth constraints, leading to improved performance.</li>
        </ul>

        <h2>Experimental Results and Plot</h2>
        <p>
            The Roofline chart below illustrates the performance of naive and SIMD implementations relative to the memory and compute roofs:
        </p>
        <img src="roofline.png" alt="Roofline Chart for INT32 Operations" />
        <p>
            Both implementations are memory-bound due to the low experimental AI (0.249985 FLOPs/byte compared to the hardware AI of 1.02 FLOPs/byte). However, SIMD achieves a significantly better runtime by efficiently utilizing the available compute resources.
        </p>
    </div>
    <footer class="footer">
        <p>&copy; 2024 Roofline Analysis Blog</p>
    </footer>
</body>
</html>
